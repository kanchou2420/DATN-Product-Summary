"""
=============================================================================
Há»† THá»NG TRAIN MÃ” HÃŒNH CHUYá»‚N TEENCODE VIá»†T NAM THÃ€NH VÄ‚N Báº¢N CHUáº¨N
=============================================================================
TÃ¡c giáº£: AI Professor
Má»¥c Ä‘Ã­ch: Text Normalization cho tiáº¿ng Viá»‡t vá»›i teencode
Kiáº¿n trÃºc: Encoder-Decoder based on mBERT + BARTpho
=============================================================================
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    BartForConditionalGeneration,
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from datasets import load_metric
import os
import re
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# BÆ¯á»šC 1: PHÃ‚N TÃCH Dá»® LIá»†U VÃ€ HIá»‚U Cáº¤U TRÃšC
# =============================================================================
print("="*80)
print("BÆ¯á»šC 1: PHÃ‚N TÃCH Dá»® LIá»†U")
print("="*80)

class DataAnalyzer:
    """
    LÃ½ do: Cáº§n hiá»ƒu cáº¥u trÃºc dá»¯ liá»‡u trÆ°á»›c khi xá»­ lÃ½
    NguyÃªn lÃ½: Exploratory Data Analysis (EDA)
    """
    def __init__(self, data_paths: Dict[str, str]):
        self.data_paths = data_paths
        
    def analyze(self):
        print("\nğŸ“Š Äang phÃ¢n tÃ­ch dá»¯ liá»‡u...")
        
        for split_name, path in self.data_paths.items():
            df = pd.read_csv(path)
            print(f"\n{split_name.upper()} Dataset:")
            print(f"  - Sá»‘ lÆ°á»£ng máº«u: {len(df)}")
            print(f"  - CÃ¡c cá»™t: {df.columns.tolist()}")
            print(f"  - VÃ­ dá»¥:")
            print(df.head(2))
            
            # PhÃ¢n tÃ­ch Ä‘á»™ dÃ i cÃ¢u
            if 'input' in df.columns and 'output' in df.columns:
                input_lens = df['input'].str.split().str.len()
                output_lens = df['output'].str.split().str.len()
                print(f"  - Äá»™ dÃ i trung bÃ¬nh input: {input_lens.mean():.1f} tá»«")
                print(f"  - Äá»™ dÃ i trung bÃ¬nh output: {output_lens.mean():.1f} tá»«")

# =============================================================================
# BÆ¯á»šC 2: Xá»¬ LÃ Dá»® LIá»†U VÃ€ CHUáº¨N HÃ“A
# =============================================================================
print("\n" + "="*80)
print("BÆ¯á»šC 2: Xá»¬ LÃ VÃ€ CHUáº¨N HÃ“A Dá»® LIá»†U")
print("="*80)

class VietnameseTextProcessor:
    """
    LÃ½ do: Dá»¯ liá»‡u thÃ´ cáº§n Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  chuáº©n hÃ³a
    Chá»©c nÄƒng:
      1. Xá»­ lÃ½ khoáº£ng tráº¯ng thá»«a
      2. Chuáº©n hÃ³a kÃ½ tá»± Ä‘áº·c biá»‡t
      3. Giá»¯ nguyÃªn emoji vÃ  icon (vÃ¬ cÃ³ Ã½ nghÄ©a trong chat)
    """
    
    @staticmethod
    def normalize_spaces(text: str) -> str:
        """Chuáº©n hÃ³a khoáº£ng tráº¯ng"""
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    @staticmethod
    def clean_text(text: str) -> str:
        """LÃ m sáº¡ch vÄƒn báº£n nhÆ°ng giá»¯ nguyÃªn cáº¥u trÃºc chat"""
        if pd.isna(text):
            return ""
        
        text = str(text)
        # Giá»¯ emoji vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t (quan trá»ng cho ngá»¯ cáº£nh chat)
        text = VietnameseTextProcessor.normalize_spaces(text)
        return text.lower()  # Chuyá»ƒn vá» lowercase Ä‘á»ƒ dá»… há»c

# =============================================================================
# BÆ¯á»šC 3: Táº O DATASET CLASS
# =============================================================================
print("\n" + "="*80)
print("BÆ¯á»šC 3: XÃ‚Y Dá»°NG PYTORCH DATASET")
print("="*80)

class TeencodeDataset(Dataset):
    """
    LÃ½ do: PyTorch yÃªu cáº§u Dataset class Ä‘á»ƒ load dá»¯ liá»‡u hiá»‡u quáº£
    NguyÃªn lÃ½: 
      - __len__: Tráº£ vá» sá»‘ lÆ°á»£ng máº«u
      - __getitem__: Tráº£ vá» 1 máº«u theo index
    Chá»©c nÄƒng:
      - Tokenize input (teencode)
      - Tokenize output (vÄƒn báº£n chuáº©n) vá»›i labels
      - Padding vÃ  truncation
    """
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 128):
        self.df = pd.read_csv(data_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.processor = VietnameseTextProcessor()
        
        # Chuáº©n hÃ³a dá»¯ liá»‡u
        print(f"ğŸ“ Loading data tá»«: {data_path}")
        self.df['input'] = self.df['input'].apply(self.processor.clean_text)
        self.df['output'] = self.df['output'].apply(self.processor.clean_text)
        
        print(f"âœ… ÄÃ£ load {len(self.df)} máº«u")
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        input_text = row['input']
        target_text = row['output']
        
        # Tokenize input
        # LÃ½ do: MÃ´ hÃ¬nh chá»‰ hiá»ƒu sá»‘, khÃ´ng hiá»ƒu chá»¯
        model_inputs = self.tokenizer(
            input_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Tokenize target
        # LÃ½ do: Decoder cáº§n labels Ä‘á»ƒ tÃ­nh loss
        labels = self.tokenizer(
            target_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Quan trá»ng: Thay padding token báº±ng -100 Ä‘á»ƒ khÃ´ng tÃ­nh loss
        # NguyÃªn lÃ½: CrossEntropyLoss bá» qua label=-100
        labels_ids = labels['input_ids'].squeeze()
        labels_ids[labels_ids == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': model_inputs['input_ids'].squeeze(),
            'attention_mask': model_inputs['attention_mask'].squeeze(),
            'labels': labels_ids
        }

# =============================================================================
# BÆ¯á»šC 4: CHá»ŒN VÃ€ Cáº¤U HÃŒNH MÃ” HÃŒNH
# =============================================================================
print("\n" + "="*80)
print("BÆ¯á»šC 4: CHá»ŒN VÃ€ Cáº¤U HÃŒNH MÃ” HÃŒNH")
print("="*80)

class ModelSelector:
    """
    LÃ½ do: Cáº§n chá»n mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i tiáº¿ng Viá»‡t
    
    So sÃ¡nh cÃ¡c lá»±a chá»n:
    
    1. BARTpho (vinai/bartpho-syllable):
       âœ… Pre-trained trÃªn tiáº¿ng Viá»‡t
       âœ… Hiá»ƒu tokenization theo Ã¢m tiáº¿t
       âœ… Seq2Seq architecture sáºµn
       âŒ Model size lá»›n hÆ¡n
       
    2. mBART (facebook/mbart-large-50):
       âœ… Multilingual, cÃ³ tiáº¿ng Viá»‡t
       âœ… Máº¡nh vá» translation
       âŒ Cáº§n fine-tune nhiá»u hÆ¡n
       
    3. mT5 (google/mt5-base):
       âœ… Text-to-Text framework
       âœ… Multilingual
       âŒ KhÃ´ng chuyÃªn vá» Viá»‡t
    
    QUYáº¾T Äá»ŠNH: DÃ¹ng BARTpho vÃ¬:
    - Pre-trained trÃªn corpus tiáº¿ng Viá»‡t lá»›n
    - Tokenizer Ã¢m tiáº¿t phÃ¹ há»£p vá»›i Viá»‡t
    - Architecture Seq2Seq sáºµn cÃ³
    """
    
    @staticmethod
    def get_model_and_tokenizer(model_name: str = "vinai/bartpho-syllable"):
        print(f"\nğŸ¤– Äang load model: {model_name}")
        print("\nNguyÃªn lÃ½ hoáº¡t Ä‘á»™ng cá»§a BART:")
        print("  - Encoder: Biáº¿n input thÃ nh hidden representations")
        print("  - Decoder: Sinh output tá»« representations + attention")
        print("  - Cross-attention: Decoder nhÃ¬n vÃ o encoder outputs")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
        print(f"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c load")
        print(f"  - Sá»‘ parameters: {model.num_parameters():,}")
        print(f"  - Vocab size: {tokenizer.vocab_size:,}")
        
        return model, tokenizer

# =============================================================================
# BÆ¯á»šC 5: THIáº¾T Láº¬P METRICS Äá»‚ ÄÃNH GIÃ
# =============================================================================
print("\n" + "="*80)
print("BÆ¯á»šC 5: THIáº¾T Láº¬P METRICS")
print("="*80)

class MetricsComputer:
    """
    LÃ½ do: Cáº§n Ä‘o lÆ°á»ng cháº¥t lÆ°á»£ng cá»§a model
    
    Metrics sá»­ dá»¥ng:
    
    1. BLEU (Bilingual Evaluation Understudy):
       - Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng n-gram giá»¯a prediction vÃ  reference
       - BLEU-1: unigram (tá»« Ä‘Æ¡n)
       - BLEU-2: bigram (cáº·p tá»«)
       - CÃ´ng thá»©c: BLEU = BP Ã— exp(Î£ wn log pn)
         + BP: Brevity Penalty (pháº¡t cÃ¢u quÃ¡ ngáº¯n)
         + pn: Precision cá»§a n-gram
       
    2. Character Error Rate (CER):
       - Äo edit distance á»Ÿ level kÃ½ tá»±
       - CER = (S + D + I) / N
         + S: Substitutions
         + D: Deletions  
         + I: Insertions
         + N: Tá»•ng sá»‘ kÃ½ tá»± trong reference
       - Quan trá»ng cho tiáº¿ng Viá»‡t vÃ¬ dáº¥u thanh
    
    3. Word Error Rate (WER):
       - TÆ°Æ¡ng tá»± CER nhÆ°ng á»Ÿ level tá»«
    """
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.bleu = load_metric("sacrebleu")
        
    def compute_metrics(self, eval_pred):
        predictions, labels = eval_pred
        
        # Decode predictions
        # LÃ½ do: Model output lÃ  token ids, cáº§n chuyá»ƒn vá» text
        decoded_preds = self.tokenizer.batch_decode(
            predictions, 
            skip_special_tokens=True
        )
        
        # Decode labels (thay -100 vá» pad token trÆ°á»›c)
        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
        decoded_labels = self.tokenizer.batch_decode(
            labels, 
            skip_special_tokens=True
        )
        
        # TÃ­nh BLEU
        # LÃ½ do: BLEU lÃ  metric phá»• biáº¿n cho machine translation
        result = self.bleu.compute(
            predictions=decoded_preds, 
            references=[[label] for label in decoded_labels]
        )
        
        # TÃ­nh accuracy Ä‘Æ¡n giáº£n (exact match)
        exact_match = sum([
            pred.strip() == label.strip() 
            for pred, label in zip(decoded_preds, decoded_labels)
        ]) / len(decoded_preds)
        
        return {
            'bleu': result['score'],
            'exact_match': exact_match * 100
        }

# =============================================================================
# BÆ¯á»šC 6: Cáº¤U HÃŒNH TRAINING
# =============================================================================
print("\n" + "="*80)
print("BÆ¯á»šC 6: Cáº¤U HÃŒNH TRAINING")
print("="*80)

class TrainingConfigurator:
    """
    LÃ½ do: Cáº§n config hyperparameters phÃ¹ há»£p
    
    Giáº£i thÃ­ch cÃ¡c tham sá»‘ quan trá»ng:
    
    1. Learning Rate (2e-5):
       - QuÃ¡ cao: Model khÃ´ng há»™i tá»¥ (oscillate)
       - QuÃ¡ tháº¥p: Há»c cháº­m, cÃ³ thá»ƒ bá»‹ stuck
       - 2e-5: GiÃ¡ trá»‹ tá»‘t cho fine-tuning BERT-based models
    
    2. Batch Size (8-16):
       - Lá»›n: Gradient stable hÆ¡n, nhÆ°ng tá»‘n RAM
       - Nhá»: Tá»‘n Ã­t RAM, nhÆ°ng noisy gradient
       - Gradient accumulation: Trick Ä‘á»ƒ tÄƒng effective batch size
    
    3. Number of Epochs (10-15):
       - QuÃ¡ Ã­t: Underfitting
       - QuÃ¡ nhiá»u: Overfitting
       - Early stopping: Dá»«ng khi validation khÃ´ng cáº£i thiá»‡n
    
    4. Weight Decay (0.01):
       - L2 regularization Ä‘á»ƒ trÃ¡nh overfitting
       - CÃ´ng thá»©c: Loss = Original_Loss + Î» Ã— Î£(weightsÂ²)
    
    5. Warmup Steps:
       - TÄƒng learning rate dáº§n tá»« 0 lÃªn max
       - GiÃºp training stable hÆ¡n á»Ÿ Ä‘áº§u
    """
    
    @staticmethod
    def get_training_args(output_dir: str = "./teencode_model"):
        print("\nâš™ï¸ Cáº¥u hÃ¬nh Training Arguments:")
        
        args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            
            # Training schedule
            num_train_epochs=15,  # Sá»‘ epoch
            per_device_train_batch_size=8,  # Batch size cho training
            per_device_eval_batch_size=8,   # Batch size cho evaluation
            
            # Optimizer settings
            learning_rate=2e-5,  # Learning rate
            weight_decay=0.01,   # L2 regularization
            warmup_steps=500,    # Warmup learning rate
            
            # Evaluation vÃ  logging
            eval_strategy="steps",  # Evaluate má»—i N steps
            eval_steps=500,         # Evaluate má»—i 500 steps
            save_steps=500,         # Save checkpoint má»—i 500 steps
            logging_steps=100,      # Log má»—i 100 steps
            
            # Early stopping vÃ  best model
            load_best_model_at_end=True,
            metric_for_best_model="bleu",
            greater_is_better=True,
            save_total_limit=3,  # Chá»‰ giá»¯ 3 checkpoints tá»‘t nháº¥t
            
            # Generation settings cho evaluation
            predict_with_generate=True,
            generation_max_length=128,
            generation_num_beams=4,  # Beam search vá»›i 4 beams
            
            # Mixed precision training (tÄƒng tá»‘c vÃ  giáº£m RAM)
            fp16=torch.cuda.is_available(),
            
            # Gradient accumulation (Ä‘á»ƒ tÄƒng effective batch size)
            gradient_accumulation_steps=2,
            
            # Report to
            report_to="none",  # CÃ³ thá»ƒ dÃ¹ng "wandb" náº¿u muá»‘n track
        )
        
        print(f"  âœ“ Output directory: {output_dir}")
        print(f"  âœ“ Epochs: {args.num_train_epochs}")
        print(f"  âœ“ Learning rate: {args.learning_rate}")
        print(f"  âœ“ Batch size: {args.per_device_train_batch_size}")
        print(f"  âœ“ Mixed precision: {args.fp16}")
        
        return args

# =============================================================================
# BÆ¯á»šC 7: XÃ‚Y Dá»°NG TRAINING PIPELINE
# =============================================================================
print("\n" + "="*80)
print("BÆ¯á»šC 7: XÃ‚Y Dá»°NG TRAINING PIPELINE")
print("="*80)

class TeencodeTrainer:
    """
    LÃ½ do: Tá»• chá»©c toÃ n bá»™ quÃ¡ trÃ¬nh training
    
    Pipeline hoáº¡t Ä‘á»™ng:
    1. Load model vÃ  tokenizer
    2. Load vÃ  preprocess data
    3. Setup trainer
    4. Train model
    5. Evaluate
    6. Save model
    """
    
    def __init__(self, data_paths: Dict[str, str]):
        self.data_paths = data_paths
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\nğŸ–¥ï¸  Device: {self.device}")
        
    def train(self):
        """Main training function"""
        
        # Step 1: Analyze data
        print("\n" + "="*80)
        print("STEP 1/7: PHÃ‚N TÃCH Dá»® LIá»†U")
        print("="*80)
        analyzer = DataAnalyzer(self.data_paths)
        analyzer.analyze()
        
        # Step 2: Load model
        print("\n" + "="*80)
        print("STEP 2/7: LOAD MODEL VÃ€ TOKENIZER")
        print("="*80)
        model, tokenizer = ModelSelector.get_model_and_tokenizer()
        model = model.to(self.device)
        
        # Step 3: Create datasets
        print("\n" + "="*80)
        print("STEP 3/7: Táº O DATASETS")
        print("="*80)
        train_dataset = TeencodeDataset(
            self.data_paths['train'], 
            tokenizer, 
            max_length=128
        )
        eval_dataset = TeencodeDataset(
            self.data_paths['dev'], 
            tokenizer, 
            max_length=128
        )
        
        # Step 4: Setup data collator
        print("\n" + "="*80)
        print("STEP 4/7: SETUP DATA COLLATOR")
        print("="*80)
        print("Data Collator:")
        print("  - Äá»™ng: Padding chá»‰ Ä‘áº¿n max length cá»§a batch (tiáº¿t kiá»‡m RAM)")
        print("  - Label smoothing: CÃ³ thá»ƒ thÃªm Ä‘á»ƒ trÃ¡nh overconfident")
        
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding=True
        )
        
        # Step 5: Setup metrics
        print("\n" + "="*80)
        print("STEP 5/7: SETUP METRICS")
        print("="*80)
        metrics_computer = MetricsComputer(tokenizer)
        
        # Step 6: Setup training arguments
        print("\n" + "="*80)
        print("STEP 6/7: Cáº¤U HÃŒNH TRAINING")
        print("="*80)
        training_args = TrainingConfigurator.get_training_args()
        
        # Step 7: Create trainer vÃ  train
        print("\n" + "="*80)
        print("STEP 7/7: Báº®T Äáº¦U TRAINING")
        print("="*80)
        
        trainer = Seq2SeqTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=metrics_computer.compute_metrics,
        )
        
        print("\nğŸš€ Báº¯t Ä‘áº§u training...")
        print("\nQuÃ¡ trÃ¬nh training:")
        print("  1. Forward pass: Input â†’ Encoder â†’ Decoder â†’ Logits")
        print("  2. Loss computation: CrossEntropyLoss(logits, labels)")
        print("  3. Backward pass: TÃ­nh gradients")
        print("  4. Optimizer step: Update weights")
        print("  5. Repeat cho má»—i batch")
        
        # Train
        train_result = trainer.train()
        
        # Save final model
        print("\n" + "="*80)
        print("ğŸ’¾ SAVING MODEL")
        print("="*80)
        trainer.save_model()
        tokenizer.save_pretrained(training_args.output_dir)
        
        print(f"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c save táº¡i: {training_args.output_dir}")
        
        # Evaluate on test set
        print("\n" + "="*80)
        print("ğŸ“Š EVALUATION ON TEST SET")
        print("="*80)
        test_dataset = TeencodeDataset(
            self.data_paths['test'], 
            tokenizer, 
            max_length=128
        )
        test_results = trainer.evaluate(test_dataset)
        
        print("\nğŸ“ˆ Test Results:")
        for key, value in test_results.items():
            print(f"  {key}: {value:.4f}")
        
        return trainer, test_results

# =============================================================================
# BÆ¯á»šC 8: INFERENCE VÃ€ DEMO
# =============================================================================

class TeencodeNormalizer:
    """
    LÃ½ do: Sá»­ dá»¥ng model Ä‘Ã£ train Ä‘á»ƒ normalize teencode
    
    Beam Search Decoding:
      - KhÃ´ng chá»n tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t má»—i bÆ°á»›c (greedy)
      - Giá»¯ k hypotheses tá»‘t nháº¥t (k=num_beams)
      - Chá»n sequence cÃ³ probability cao nháº¥t tá»•ng thá»ƒ
      - VÃ­ dá»¥ vá»›i beam=3:
        Step 1: "tÃ´i" | "mÃ¬nh" | "em"
        Step 2: "tÃ´i Ä‘ang" | "mÃ¬nh Ä‘ang" | "tÃ´i Ä‘i"
        ...
    """
    
    def __init__(self, model_path: str):
        print(f"\nğŸ”„ Loading model tá»« {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.model.to(self.device)
        self.model.eval()
        print("âœ… Model ready!")
        
    def normalize(self, text: str, num_beams: int = 4) -> str:
        """
        Normalize teencode text
        
        Args:
            text: Input teencode
            num_beams: Sá»‘ beams cho beam search (cÃ ng lá»›n cÃ ng tá»‘t nhÆ°ng cháº­m)
        """
        # Tokenize
        inputs = self.tokenizer(
            text, 
            return_tensors='pt', 
            max_length=128, 
            truncation=True,
            padding=True
        ).to(self.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=128,
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=2,  # TrÃ¡nh láº·p n-gram
                length_penalty=1.0,       # Penalty cho Ä‘á»™ dÃ i
            )
        
        # Decode
        normalized = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return normalized

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main function"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                       â•‘
    â•‘     ğŸ“ TRAINING TEENCODE NORMALIZATION MODEL FOR VIETNAMESE ğŸ‡»ğŸ‡³        â•‘
    â•‘                                                                       â•‘
    â•‘     Kiáº¿n trÃºc: BART-based Sequence-to-Sequence                       â•‘
    â•‘     Task: Text Normalization (Lexical Normalization)                 â•‘
    â•‘     Dataset: ViLexNorm                                               â•‘
    â•‘                                                                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Data paths
    data_paths = {
        'train': r'C:\Users\maxez\OneDrive\Documents\DATN-Product-summary\DATN-Product-Summary\datateencode\train.csv',
        'dev': r'C:\Users\maxez\OneDrive\Documents\DATN-Product-summary\DATN-Product-Summary\datateencode\dev.csv',
        'test': r'C:\Users\maxez\OneDrive\Documents\DATN-Product-summary\DATN-Product-Summary\datateencode\test.csv'
    }
    
    # Kiá»ƒm tra files tá»“n táº¡i
    print("\nğŸ” Kiá»ƒm tra files...")
    for name, path in data_paths.items():
        if os.path.exists(path):
            print(f"  âœ… {name}: {path}")
        else:
            print(f"  âŒ {name}: File khÃ´ng tá»“n táº¡i!")
            return
    
    # Initialize trainer
    trainer = TeencodeTrainer(data_paths)
    
    # Train
    model, results = trainer.train()
    
    # Demo inference
    print("\n" + "="*80)
    print("ğŸ¯ DEMO INFERENCE")
    print("="*80)
    
    normalizer = TeencodeNormalizer("./teencode_model")
    
    test_cases = [
        "k biet lam sao nua",
        "vs cau ay thi minh cx chiu",
        "ck ay bua qua di",
        "hom wa minh di choi vui vcl"
    ]
    
    print("\nKáº¿t quáº£ normalize:")
    for teencode in test_cases:
        normalized = normalizer.normalize(teencode)
        print(f"\n  Input:  {teencode}")
        print(f"  Output: {normalized}")
    
    print("\n" + "="*80)
    print("âœ… HOÃ€N THÃ€NH!")
    print("="*80)
    print(f"\nModel Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: ./teencode_model")
    print("\nCÃ¡ch sá»­ dá»¥ng model:")
    print("""
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
    
    tokenizer = AutoTokenizer.from_pretrained('./teencode_model')
    model = AutoModelForSeq2SeqLM.from_pretrained('./teencode_model')
    
    text = "k biet lam sao"
    inputs = tokenizer(text, return_tensors='pt')
    outputs = model.generate(**inputs)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    """)

if __name__ == "__main__":
    main()